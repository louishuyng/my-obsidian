---
tags: Database, Scale
---
Never talk about something you do not fully understand during an interview

Another interview story.

Q: â€œIntroduce yourself and the recent project you are working on, as detail as possibleâ€.

A: â€œSure, Iâ€™m â€¦, working at an internet company, a public-facing platform that serves millions of requests per minute..blabla.. my team is a search module, and we provide search services to other teams.â€

Q: â€œOk, what technology is used in your team, and what is your scope, any challenges you have been facing, and how have you solved themâ€.

A: â€œblablaâ€¦ by the way we scaled our database (MySQL) by sharding. it is fast and scalableâ€.

Q: â€œInteresting, why go for sharding?â€

A: â€œIt is a big table. data keep increasing in a single table, search becomes slow as it grows, we need them to be split into different machines. so the search is faster and scales horizontallyâ€

Q: â€œOkay. how do you shard it, range-based or key based? If key, what is the sharding key?â€

A: â€œWe use hash, sharding key is xx.â€

Q: â€œCould you tell me how to query the recent 500th to 1000th updated records and order by modified date? â€

A: â€œâ€¦.â€ # after 3 minutes

Q: â€œYou have to query all databases and order by modified (must be indexed well) and limit 500 offsets 500 then merge in memory right?â€ I tried to continue the conversation.

A: â€œNo we have our own algorithm to do it more efficientlyâ€¦â€

He drew something on the whiteboard Id [0,5000]-machineA, Id[5001, 10000]-machineB, â€¦ then I reminded him â€œyou told me your sharding key is hash right ğŸ˜„?â€, he stopped and spent a few minutes thinking, didnâ€™t come up any more answer.

Then we discussed something else, and the interview finished in another 3 mins.

To this topic, there are much more difficult questions ahead in my mind:

- When you add a new machine, do you need re-shard? and how did you handle it?
- How do you handle schema changes?
- How do you join tables? Does your code have no sub-query? and what if you have to use it? Is there an alternative approach?
- How do you make sure data integrity? after you did sharding, you threw away the entire ACID (primary key only applies to local shard table) which came from RDBMS: data loss and inconsistency now become possible.
- And all the aggregating functions like count, sum, and max become challenging, there must be a high-performance and mature sharding proxy or â€œmerging engineâ€ likeÂ [vitess](https://vitess.io/docs/)Â orÂ [shardingsphere](https://github.com/apache/shardingsphere)Â in the middle. and you need to master this engine to save your ass.
- How is the transaction handling different from a single database?
- â€¦

Never talk about something you are not sure about during an interview. ğŸ™‚

Today letâ€™s spend a few minutes talking about scaling the database.

First thing, you will not want to go sharding in the first place.

Unless you tried all the possible options, otherwise, never shard your database (even for NoSQL which naturally support distributed system, do not shard until you have to).

Why?

It makes things super complicated.

So letâ€™s follow this intervieweeâ€™s problem, he has a database table, that keeps growing, and makes queries slow, how to solve it?

# First, split tables

Yes, sharding, but vertically.

To mitigate the table growing issue, we need to split the columns into 2 categories:

- Frequently changes (red)
- No or fewer changes (blue)

So that we can eliminate tracking those unchanged rows.

![](https://miro.medium.com/v2/resize:fit:1306/1*zr0c-xfU71AzJM_bAwSo-g.png)

So letâ€™s say it is an online shopping system. We have a product table with some columns rarely change after first creation, likeÂ _id, name photo_. and some columns that may need to be updated frequently likeÂ _supplier or warehouse_id and verified employee id_.

by splitting it into 2 tables. we reduced the writes into the product_basic table.

So indirectly we reduced the potential locks on the old big table, and improved the performance.

Okay you may ask â€œWait. this is the problem: we have a table, that keeps growing, and using the above solution will still end up with a huge product_etx tableâ€

Sure we can continue to try something else.

# Then, try the simplest way first, to Archive

Keep the busy database small.

==Archive the old records (split the cold and hot data). create an archiving database and move data over there every month or year.==

Do not forget to compress the files if your database supports this feature.

So if you need to search historical data, go to this archive database, if you find the schema is not what you want, you can even create a reporting database and flow the data there for your purpose (you can build an ETL in between if needed).

So you may ask â€œWait, What if after archiving it is still slow, because we can only archive 5 years ago dataâ€.

Sure. we try something, and no still not sharding.

# Read-Write Splitting

![](https://miro.medium.com/v2/resize:fit:1264/1*j8oiqeGGAjbiOLCd3CM3XA.png)

- Master-Slave + replica
- Read goes to slaves, Write into master
- One way sync master to slaves (most database support this)

If archiving itself doesnâ€™t solve the problem. on top of that, we split read/write.

You may want to check out CQRS + event sourcing which is a very mature proven working solution for years.

You may wonder What if this case â€œvery high concurrency (10k+ RPS) happens, so race-conditioning the same rows causes a lot of deadlocks. it may even kill MySQL.â€

Sure, you can add a cache for that high frequent accessed table data in front to protect the database.

But since you introduced cache then remember to write through or use invalidate date to control the expiration of the caching data.

And if needed, make a cache cluster as well.

â€œNo, the cache only deal with the concurrency but didnâ€™t do anything to the fast-growing table, archiving too much data is not an option, and read-write is not enough to get the performance.â€

Well. okay. letâ€™s try one more.

# Table Partition

![](https://miro.medium.com/v2/resize:fit:1318/1*08G7EiJFwpddMfOb9JgxUQ.png)

By time range. the data will be split into different partition physical files and all are managed by MySQL.

Yes the limitation, or the difference from sharding (range based) is, all partitions are on the same machine.

You may ask â€Wait, It didnâ€™t solve the issue, right? it is not scalable. I can not scale the solution by adding a new machine.â€

but you can add a new disk, right?

â€œIn the end, the machine will be a bottleneckâ€.

Well, true, you are right. letâ€™s continue.

# Sharding, but range based

![](https://miro.medium.com/v2/resize:fit:1340/1*eqH2cV_Cjm8jRv29MdzPRg.png)

In our application logic layer, we can route the data to different databases based on logical ids.

It could beÂ _customer_id or user_id, time range, location_id, or city_id, etc._Â the load will be balanced at the application layer.

If one of the machines has a problem, only affects the particular location or city or range of customers.

This solution is scalable, balanced the load, and also splits the risk.

Okay you may ask â€what if in my case, the single location or city_id data grows still very very huge? Yes, I mean the data hotspot case, so we need another solution.â€

Well, you should choose the sharding key very very carefully to avoid this happeningâ€¦

Btw, this is the max level of complexity I could accept for a database solution. beyond which is a bit out of control.

# Key-based sharding

Letâ€™s say 2 machines. the hashing function is as simple as row_id%2. row_id%2==0 go to machine1 and another one go machine2

![](https://miro.medium.com/v2/resize:fit:1400/1*S-jfRXHuBRWu0YKAoQU_EA.png)

It only looks simple, but If possible, I never want to reach here.

The â€œconsâ€ make me stop thinking about it.

**Cons**

- I can never use incremental Id as the primary key.
- Primary or foreign keys and unique keys do not make sense anymore. it is only applied to the shard database table. we need another solution to get this feature back.
- The hashing function needs to be chosen very very carefully. e.g. For the above sample, when I add a machine, need to change the function into row_id%3. which is very very bad â€” re-shard is the cost is very very high: migration is complicated, potentially may lose data, and the downtime is unknown.
- Need to rewrite many queries (into very simple ones). no subquery, no joins, and no aggregate functions can be used.
- The transaction is hard to control.Â [distributed system transaction is already complicated](https://iorilan.medium.com/i-asked-this-system-design-question-to-3-guys-during-a-developer-interview-and-none-of-them-gave-9c23abe45687), sharding makes it even harder.
- When a slow query (after a query from multiple machines then merges and sorts in memory) happens, my luck depends on caching or the engine in the middle likeÂ [vitess](https://vitess.io/docs/)Â orÂ [shardingsphere](https://github.com/apache/shardingsphere).
- With above. horizontal sharding seems more suitable for NoSQL. but I donâ€™t try until I have to.

**Pros (compared to range based)**

- No data hotspot problem
- Nothing more -_-!

So, we may still want to know, â€œI know all the cons, what if we have to use itâ€. so when this happens, donâ€™t build it yourself, even if you are confident to handle all the above â€œconsâ€.

- Try with NoSQL first. it is naturally distributed and mostly has more built-in features to support sharding
- If you have to use SQL like MySQL. use some proven engine like vitess (used by youtube).Â [here is an example of how slack did it.](https://slack.engineering/scaling-datastores-at-slack-with-vitess/)
- If doesnâ€™t work, try another engine, keep trying, and use the existing solution, never build it yourself.

# Recap

when scaling a big table with growing data.

#1 Try the simplest way first

#2 It does not work then slowly adds complexity

#3 Keep balancing the benefits that the complexity brings.

#4 Never jump into any complicated solutions (even if it looks sexy).

- Scale up: Add RAM/CPU (CPU bound) or use SSD (I/O bound). whenever possible.
- Reuse existing mature proven solutions. e.g. handle growing logs searching, throwing them into ELK.
- Review table fields. categorize the fields by updating frequency. put those fields updated more frequently into one table and others into another table.
- Archive history data + Split read and write + replica + Cache (In many cases, we stopped here, all below solutions has risk)
- Table partition (by date range). â€” risk: be mindful of the physical file size and the usage of an information schema.
- Sharding, range based (choose the key carefully, avoid hotspot data) â€” risk: think carefully when choosing the key, avoid hotspot. and if this happens, changing back to key-based is almost impossible.
- Is the data structure could be fit into any NoSQL? migrate there â€” risk: mysqldump is not possible. select into outfile csv then import seems the only way, to benchmark more and test all languages, and special chars.
- Shard by key. check Vitess and other existing proven shard engines. setup and reuse. â€” risk: Sorry I donâ€™t know what could happen to you, good luck. :)